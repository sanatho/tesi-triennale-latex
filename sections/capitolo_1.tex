\chapter{Introduction}
\thispagestyle{empty}
\section{Autonomous Surface Vehicles: challenges and applications}
Unmanned Surface Vehicles (\textbf{USVs}) are particular surface vessels capable of navigating without the presence of a human crew to maneuver them. They can be remotely operated through radio or satellite communication, or be completely autonomous and controlled by a computer or artificial intelligence; in this case, they are referred to as \textbf{Autonomous Surface Vehicles (ASVs)}. ASVs aim to make journeys safer, reduce costs, increase operational continuity, and carry out prolonged missions in complex and even risky situations for humans.

All this has been made possible thanks to significant technological evolution in \textbf{sensors} (satellite, IMU, cameras, LIDAR, radar, etc.), major developments in robotics libraries, and onboard computational power that is increasingly greater and more affordable. Despite this, the marine environment presents major challenges. For example, the plane of the sea surface is not a fixed reference but changes independently of the boat’s movements; the surface creates reflections that can interfere with instrumentation; and wind and waves can put stability control systems to the test. In the marine context, therefore, attitude estimation (roll, pitch, and yaw) is essential, as it is decisive both for the success of the journey and for the precision required in maneuvers, such as docking.

The main challenges concern \textbf{sensors and algorithms}. The first challenge involves navigation with GNSS (Global Navigation Satellite System), which degrades in proximity to port infrastructures, bridges, or areas with many vertical obstacles, as part of the visible satellites are lost. This problem has been partly mitigated with inertial sensors (IMU), which, however, suffer from another issue: drift. The second challenge is the detection of possible obstacles at sea, such as buoys, debris, or even other vessels, often under conditions of reduced visibility or reflections. The third challenge is control and stability, made complex by wave motion that often causes very rapid variations. The last challenge is the ability to make real-time decisions, including very quick changes dictated by sudden shifts in the operational situation.

Despite the criticality of these challenges, ASVs are used in \textbf{various applications}. In the industrial field, they are employed for infrastructure inspection or short-range logistical support. In defense and security, they are used in special operations such as explosive ordnance disposal. In the environmental field, they are used for monitoring environmental parameters and collecting meteorological data over extended time horizons, thereby reducing risks for crews.

Within this framework, the present thesis focuses on one main issue: obtaining an \textbf{accurate real-time estimation} of the boat’s attitude. To achieve this, computer vision techniques (stereo vision and ArUco markers), inertial measurements through the IMU, and an Extended Kalman Filter will be used to fuse the data, with the aim of improving the reliability of the estimation.

\section{Attitude Estimation: roll, pitch, yaw}

To describe the \textbf{attitude} of a vessel, the orientation of the rigid body with respect to a reference system is used. This is done through the three \textbf{Euler angles}\cite{Euler_angles}: roll (rotation on the longitudinal axis), pitch (rotation on the transverse axis), and yaw (rotation on the vertical axis). These three quantities are crucial as they influence stabilization and trajectory control and are fundamental in precision maneuvers.  

\begin{figure}[ht]
  \centering
  \includegraphics[height=6cm]{images/euler_angles.png}
  \caption{Three rotational degrees of freedom of a ASV}\label{unipd-logo}
\end{figure}


\textbf{Attitude estimation} can be obtained from different sources, each with its own advantages and disadvantages. Inertial sensors (IMU)\cite{IMU_Euler}, through gyroscopes and accelerometers, provide roll and pitch with a high update frequency. However, these estimates are affected by drift, caused by the accumulation of errors over time. For yaw, a magnetometer or an external observation is required, as the IMU does not provide a direct measurement of this angle. A visual sensor allows estimating the attitude through known structures (e.g., ArUco tags) or through the environment if it allows it (e.g., using the horizon line). This latter method is less affected by the drift problem but is highly sensitive to environmental conditions and also has a lower update frequency.  
\\In the marine context, a robust pipeline is represented by the IMU, which provides a high-frequency prediction and, periodically, through computer vision, a measurement relative to the ArUco marker to reduce error accumulation. Through an EKF filter\cite{EKS_IMU_cv}, the different contributions are integrated in order to obtain a coherent estimate even if the conditions are adverse or if one of the sources fails.  
\\In summary, attitude estimation for an \textbf{ASV} requires a compromise between the speed of the readings and their accuracy. The integration between inertial and visual measurements provides a reliable estimate capable of supporting autonomous navigation. Subsequently, the different implementation choices to obtain the attitude estimation of the \textbf{Blue Boat} will be presented.

\section[Computer Vision and Attitude Estimation]{Computer Vision for Localization and Attitude Estimation}

Computer vision is an essential source for estimating the attitude of the ASV. 
It provides roll, pitch, and yaw with respect to a known reference, namely the \textit{ArUco marker}. 
In our case, it plays a complementary role with respect to the IMU, 
which provides an absolute measurement that is not obtained through temporal integration, 
but is instead sensitive to visual conditions (reflections, backlighting, fog, etc.). 
The goal of this chapter is to outline the principles and choices that allow for reliable 
attitude estimation using a stereo camera pair, leveraging \textit{ArUco markers} 
and a geometric calibration of the two cameras.

The pipeline is structured into four main steps. First, a stereo camera calibration is performed to estimate the intrinsic parameters and distortions, followed by the calculation of rotation and translation between the two cameras. Then, marker detection is carried out through ArUco markers, which enable highly reliable identification provided they are properly scaled according to the distance and have a sufficient pixel resolution. Subsequently, attitude estimation is obtained by applying a pose estimation algorithm which, knowing the 2D corners and the 3D points of the pattern, can estimate the values of roll, pitch, and yaw. Finally, a measurement validation step is applied, where a decision filter determines whether to accept the measurement, discard it, or reduce its influence.

Triangulation, made more reliable by epipolar rectification, allows for the estimation of depth around the marker. 
These depth cues are used as additional support to improve the attitude estimation.

Finally, temporal and synchronization aspects also play a crucial role. 
Synchronization is decisive to avoid disparities and inconsistent triangulations. 
The frame rate and the subsequent update time strongly influence the accuracy of the algorithm’s estimation, 
since excessively low values reduce its quality.

This approach, entirely based on stereo vision and ArUco markers, enables precise attitude estimation. 
Its accuracy is determined by the quality of calibrations and the robustness of estimation algorithms, 
but it reaches its maximum effectiveness when integrated with IMU data, 
compensating for the weaknesses of each source.

\section{Fiducial markers and ArUco tags}

The ArUco marker is a square planar pattern with a high-contrast black border and an internal matrix encoding a unique identifier. This structure allows for fast localization and recognition of the four corners, providing a stable 2D--3D correspondence for attitude estimation. The use of a single marker reduces environmental and economic requirements. However, relying on just one marker requires greater care in localization, calibration, and validation of the measurements.

The recognition pipeline adopts a pair of calibrated cameras. After calibration of the individual cameras and subsequent stereo calibration (\textit{rotation}, \textit{translation}, and \textit{epipolar rectification}), the images are acquired and rectified, thus aligning the epipolar lines. The detection of the marker involves several steps: \textbf{adaptive thresholding}, contour extraction, quadrilateral selection, and ID decoding.
\\Knowing the marker size, the attitude is calculated through the \textbf{estimateAffine3D method}, obtaining the rotation and translation of the reference camera frame. If both cameras observe the marker, the two estimates can be fused, or the one with higher quality can be selected.
\\The use of a system based on \textbf{stereo vision} improves disparity estimation in the regions close to the marker and enables metric triangulation of depth, enhancing stability compared to a monocular approach. This becomes particularly evident when the marker is very small (i.e., far away) or when it is not perpendicular to the viewing angle, as both conditions reduce perspective information.
\\For each estimate, quality indicators are evaluated: reprojection error, effective presence of the marker (all four corners detected), and the angle between the camera and the marker, penalizing views that are too oblique. Furthermore, a comparison is made between the apparent size of the marker in pixels, in order to identify discrepancies due to triangulation or temporal misalignment.
\\From a practical point of view, the use of a single ArUco marker offers a simple and easily reproducible implementation path: a single print on a rigid surface with a matte finish (to reduce reflections) is sufficient. Under these conditions, a single ArUco marker used in a well-calibrated stereo vision pipeline provides roll, pitch, and yaw measurements while maintaining a good trade-off between precision and computational demand.
\section{Sensor Fusion and Bayesian Approaches}

Sensor fusion deals with solving the problem of combining heterogeneous and noisy measurements in order to obtain a state estimate that is more accurate than the one provided by individual sensors. In our case, we merge sensors with complementary properties. Stereo vision provides absolute measurements without drift but at a low frequency, while IMU provides angular velocity measurements at high frequency but suffers from drift.


\subsection*{Bayesian Foundations}
In the Bayesian paradigm, the state is described by a probability distribution that evolves over time and is composed of two steps. The prediction step computes the next state based on the dynamic model and the noise statistics, while the update step uses the available measurement through the observation function and the measurement noise statistics. This alternation achieves an optimal trade-off between what is computed and what is effectively measured. 
\\In our case, the state consists of \textbf{Euler angles} (roll, pitch, yaw), the dynamics are modeled by integrating angular velocities to predict the evolution of the angles using consecutive samples, and the observations come from orientation measurements derived from vision and inertial sensors, which are treated either as model input or as an observation depending on the architecture. It is also necessary to guarantee \textbf{numerical consistency} between prediction and update, managing, for example, the correct periodicity of measurements and possible discontinuities among angles.

\subsection*{Benefits of Fusion}
Fusion provides several benefits. It reduces drift because vision-based observations remove the accumulation of error caused by inertial integration. The temporal continuity of the IMU keeps the estimate stable due to its high update frequency. \\Fusion also improves robustness, as weighting measurements based on their uncertainty allows discarding unreliable measurements from stereo vision (e.g., when the camera is at a steep angle relative to the ArUco marker or when the marker is too small) or from the IMU.


\subsection*{Evaluation}
The quality of the fusion evaluation is assessed based on three main aspects: \textbf{angular accuracy}, \textbf{temporal stability}, and \textbf{behavior under varying visual quality}.


\subsection*{Summary}
In summary, \textbf{sensor fusion with a Bayesian algorithm} allows combining stereo vision and IMU measurements to obtain the attitude estimate. The IMU provides a continuous and fast estimate, while stereo vision provides an estimate that does not suffer from drift since it is anchored to fixed references.


Weighting the measurements according to their uncertainty allows operation even when the quality of observations is not uniform.

\section[Kalman Filters as Bayesian Filters]{Kalman Filters as Bayesian Filters (Linear and Extended)}

Kalman filters are part of the family of \textbf{Bayesian filters}; they recursively estimate the state of the system in a probabilistic way and update it through a dynamic model (prediction) with new observations (update) using Bayes’ theorem. In linear cases with Gaussian noise of zero mean, the \textbf{Kalman Filter (KF)} provides an \textbf{optimal solution} since it maintains the state described by mean and covariance, producing a Kalman gain that balances the model and the measurements. In nonlinear systems, typical in robotics and computer vision, the \textbf{Extended Kalman Filter (EKF)} is used, which applies a local linearization (Jacobian matrices) of the models to preserve the prediction structure.

In our case, to estimate the attitude, the minimum state includes the \textbf{Euler angles (roll, pitch, yaw)} and the \textbf{angular velocities}. The prediction integrates angular velocities over the sampling step, while the update incorporates an orientation observation coming from computer vision, with a covariance built from the quality indicators of the stereo vision pipeline (reprojection error, number and quality of corners, etc.). In this case, the filter avoids drift caused by the inertial system if the computer vision part is stable.

At the operational level, the linear KF, through known transition and observation matrices, calculates prediction and update. If the state is nonlinear, the \textbf{EKF} is used, which, through the calculation of Jacobian matrices, replaces constant matrices within the same recursive scheme. The success of the fusion depends on the consistency of the measurements, which in turn depends on temporal synchronization between computer vision and IMU measurements, as well as good performance conditions that allow everything to run in real time.

In summary, the \textbf{Kalman Filter} and its extended version, through a Bayesian approach, perform sensor fusion in a computationally efficient way: the prediction exploits the dynamic model and the IMU angular velocities to ensure continuity and responsiveness in any situation, while the update also integrates computer vision, thus providing an accurate and temporally stable attitude estimate.
